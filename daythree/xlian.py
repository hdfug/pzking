import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
import os
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm


# 1. æ•°æ®å‡†å¤‡
def prepare_data(data_dir='C:/Users/å½­å­æ˜‚/PycharmProjects/suanfa3/daythree/Images', batch_size=64, val_ratio=0.2):
    # æ•°æ®å¢å¼ºå’Œå½’ä¸€åŒ–
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # åŠ è½½å®Œæ•´æ•°æ®é›†
    full_dataset = ImageFolder(root=data_dir, transform=transform)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    return train_loader, val_loader, full_dataset.classes


# 2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ (ä½¿ç”¨ResNet50é¢„è®­ç»ƒæ¨¡å‹)
class ImageClassifier(nn.Module):
    def __init__(self, num_classes=100):
        super(ImageClassifier, self).__init__()
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet50
        self.base_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)

        # å†»ç»“æ‰€æœ‰å·ç§¯å±‚ (å¯é€‰)
        # for param in self.base_model.parameters():
        #     param.requires_grad = False

        # æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
        in_features = self.base_model.fc.in_features
        self.base_model.fc = nn.Sequential(
            nn.Linear(in_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.base_model(x)


# 3. è®­ç»ƒå‡½æ•°
def train_model(train_loader, val_loader, num_classes, epochs=25, lr=0.001):
    # åˆå§‹åŒ–æ¨¡å‹å¹¶ç§»è‡³GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ImageClassifier(num_classes=num_classes).to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    # TensorBoardè®°å½•
    writer = SummaryWriter()

    best_acc = 0.0

    for epoch in range(epochs):
        print(f'Epoch {epoch + 1}/{epochs}')
        print('-' * 10)

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in tqdm(train_loader, desc='Training'):
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            with torch.set_grad_enabled(True):
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                loss.backward()
                optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        scheduler.step()

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = evaluate_model(model, val_loader, device, criterion)

        # è®°å½•åˆ°TensorBoard
        writer.add_scalars('Loss', {'train': epoch_loss, 'val': val_loss}, epoch)
        writer.add_scalars('Accuracy', {'train': epoch_acc, 'val': val_acc}, epoch)

        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')

    writer.close()
    return model


# 4. è¯„ä¼°å‡½æ•°
def evaluate_model(model, data_loader, device, criterion):
    model.eval()
    running_loss = 0.0
    running_corrects = 0

    for inputs, labels in tqdm(data_loader, desc='Validating'):
        inputs = inputs.to(device)
        labels = labels.to(device)

        with torch.set_grad_enabled(False):
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    total_loss = running_loss / len(data_loader.dataset)
    total_acc = running_corrects.double() / len(data_loader.dataset)

    return total_loss, total_acc


# 5. ä¸»å‡½æ•°
def main():
    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        print(f"ğŸš€ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ è­¦å‘Š: å°†ä½¿ç”¨CPUè®­ç»ƒï¼Œé€Ÿåº¦ä¼šè¾ƒæ…¢")

    # å‡†å¤‡æ•°æ®
    train_loader, val_loader, classes = prepare_data()
    print(f"ğŸ“Š æ•°æ®é›†åŒ…å« {len(classes)} ä¸ªç±»åˆ«")

    # è®­ç»ƒæ¨¡å‹
    trained_model = train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        num_classes=len(classes),
        epochs=25,
        lr=0.001
    )

    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == '__main__':
    main()#